"""
Sequence-to-Sequence Summarizer Trainer

This module provides functionality to train a sequence-to-sequence model for movie plot summarization
using data generated from the LLMSummarizer. It leverages the Hugging Face transformers library
to fine-tune pre-trained models for the summarization task.

The Seq2SeqSummarizerTrainer class can:
1. Load summaries generated by the LLMSummarizer
2. Prepare the data for training a sequence-to-sequence model
3. Define and train a sequence-to-sequence model
4. Evaluate the model
5. Save and load the trained model
"""

import os
import json
import pandas as pd
import numpy as np
import torch
import logging
from typing import List, Dict, Any, Optional, Union, Tuple
from sklearn.model_selection import train_test_split
from transformers import (
    AutoTokenizer, 
    AutoModelForSeq2SeqLM,
    Seq2SeqTrainingArguments,
    Seq2SeqTrainer,
    DataCollatorForSeq2Seq
)
from datasets import Dataset
from tqdm.auto import tqdm

# Set up logging
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(name)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

class Seq2SeqSummarizerTrainer:
    """
    A class for training sequence-to-sequence models for movie plot summarization
    using data generated from the LLMSummarizer.
    """
    
    def __init__(self, 
                 model_name: str = "t5-small",
                 max_input_length: int = 512,
                 max_output_length: int = 128,
                 device: Optional[str] = None):
        """
        Initialize the Seq2SeqSummarizerTrainer.
        
        Parameters:
        -----------
        model_name : str, default='t5-small'
            Name of the pre-trained model to use (e.g., 't5-small', 'facebook/bart-base')
        max_input_length : int, default=512
            Maximum length of input sequences
        max_output_length : int, default=128
            Maximum length of output sequences
        device : str, optional
            Device to use for training ('cuda', 'cpu'). If None, will use CUDA if available.
        """
        self.model_name = model_name
        self.max_input_length = max_input_length
        self.max_output_length = max_output_length
        
        # Set device
        if device is None:
            self.device = "cuda" if torch.cuda.is_available() else "cpu"
        else:
            self.device = device
            
        logger.info(f"Using device: {self.device}")
        
        # Initialize tokenizer and model
        self.tokenizer = AutoTokenizer.from_pretrained(model_name)
        self.model = AutoModelForSeq2SeqLM.from_pretrained(model_name).to(self.device)
        
        # Initialize data containers
        self.train_dataset = None
        self.eval_dataset = None
        self.test_dataset = None
        self.trainer = None
        
        logger.info(f"Initialized Seq2SeqSummarizerTrainer with model: {model_name}")
    
    def load_summaries_from_file(self, file_path: str) -> Dict[str, Dict[str, Any]]:
        """
        Load summaries from a JSON file generated by LLMSummarizer.
        
        Parameters:
        -----------
        file_path : str
            Path to the JSON file containing summaries
            
        Returns:
        --------
        dict : Loaded summaries
        """
        if not os.path.exists(file_path):
            raise FileNotFoundError(f"File not found: {file_path}")
        
        with open(file_path, 'r', encoding='utf-8') as f:
            summaries = json.load(f)
        
        logger.info(f"Loaded {len(summaries)} summaries from {file_path}")
        return summaries
    
    def prepare_data_from_summaries(self, 
                                   summaries: Dict[str, Dict[str, Any]],
                                   test_size: float = 0.1,
                                   val_size: float = 0.1,
                                   random_state: int = 42) -> Tuple[Dataset, Dataset, Dataset]:
        """
        Prepare training, validation, and test datasets from summaries.
        
        Parameters:
        -----------
        summaries : dict
            Dictionary of summaries from LLMSummarizer
        test_size : float, default=0.1
            Proportion of data to use for testing
        val_size : float, default=0.1
            Proportion of data to use for validation
        random_state : int, default=42
            Random seed for reproducibility
            
        Returns:
        --------
        tuple : (train_dataset, val_dataset, test_dataset)
        """
        # Convert summaries to DataFrame
        data = []
        for title, content in summaries.items():
            data.append({
                'title': title,
                'plot': content['original_plot'],
                'summary': content['summary']
            })
        
        df = pd.DataFrame(data)
        
        # Split data into train, validation, and test sets
        train_df, test_df = train_test_split(df, test_size=test_size, random_state=random_state)
        train_df, val_df = train_test_split(train_df, test_size=val_size/(1-test_size), random_state=random_state)
        
        logger.info(f"Data split: {len(train_df)} train, {len(val_df)} validation, {len(test_df)} test")
        
        # Convert to Hugging Face datasets
        train_dataset = Dataset.from_pandas(train_df)
        val_dataset = Dataset.from_pandas(val_df)
        test_dataset = Dataset.from_pandas(test_df)
        
        # Preprocess datasets
        train_dataset = train_dataset.map(
            self._preprocess_function,
            batched=True,
            remove_columns=['title', 'plot', 'summary', '__index_level_0__']
        )
        val_dataset = val_dataset.map(
            self._preprocess_function,
            batched=True,
            remove_columns=['title', 'plot', 'summary', '__index_level_0__']
        )
        test_dataset = test_dataset.map(
            self._preprocess_function,
            batched=True,
            remove_columns=['title', 'plot', 'summary', '__index_level_0__']
        )
        
        self.train_dataset = train_dataset
        self.eval_dataset = val_dataset
        self.test_dataset = test_dataset
        
        return train_dataset, val_dataset, test_dataset
    
    def _preprocess_function(self, examples):
        """Preprocess the examples for sequence-to-sequence training."""
        # Prepare inputs
        inputs = examples['plot']
        model_inputs = self.tokenizer(
            inputs, 
            max_length=self.max_input_length, 
            padding="max_length", 
            truncation=True
        )
        
        # Prepare targets
        outputs = examples['summary']
        with self.tokenizer.as_target_tokenizer():
            labels = self.tokenizer(
                outputs, 
                max_length=self.max_output_length, 
                padding="max_length", 
                truncation=True
            )
        
        model_inputs["labels"] = labels["input_ids"]
        return model_inputs
    
    def train(self, 
             output_dir: str = "seq2seq_summarizer_model",
             num_train_epochs: int = 3,
             per_device_train_batch_size: int = 8,
             per_device_eval_batch_size: int = 8,
             warmup_steps: int = 500,
             weight_decay: float = 0.01,
             logging_dir: str = "logs",
             logging_steps: int = 100,
             evaluation_strategy: str = "epoch",
             save_strategy: str = "epoch",
             load_best_model_at_end: bool = True,
             metric_for_best_model: str = "eval_loss",
             greater_is_better: bool = False) -> Seq2SeqTrainer:
        """
        Train the sequence-to-sequence model.
        
        Parameters:
        -----------
        output_dir : str, default='seq2seq_summarizer_model'
            Directory to save the model
        num_train_epochs : int, default=3
            Number of training epochs
        per_device_train_batch_size : int, default=8
            Batch size for training
        per_device_eval_batch_size : int, default=8
            Batch size for evaluation
        warmup_steps : int, default=500
            Number of warmup steps
        weight_decay : float, default=0.01
            Weight decay
        logging_dir : str, default='logs'
            Directory for logs
        logging_steps : int, default=100
            Number of steps between logging
        evaluation_strategy : str, default='epoch'
            When to evaluate ('no', 'steps', 'epoch')
        save_strategy : str, default='epoch'
            When to save checkpoints ('no', 'steps', 'epoch')
        load_best_model_at_end : bool, default=True
            Whether to load the best model at the end of training
        metric_for_best_model : str, default='eval_loss'
            Metric to use for best model selection
        greater_is_better : bool, default=False
            Whether higher is better for the metric
            
        Returns:
        --------
        Seq2SeqTrainer : Trained trainer
        """
        if self.train_dataset is None or self.eval_dataset is None:
            raise ValueError("Datasets not prepared. Call prepare_data_from_summaries() first.")
        
        # Define training arguments
        training_args = Seq2SeqTrainingArguments(
            output_dir=output_dir,
            num_train_epochs=num_train_epochs,
            per_device_train_batch_size=per_device_train_batch_size,
            per_device_eval_batch_size=per_device_eval_batch_size,
            warmup_steps=warmup_steps,
            weight_decay=weight_decay,
            logging_dir=logging_dir,
            logging_steps=logging_steps,
            eval_strategy=evaluation_strategy,
            eval_steps=500,
            save_strategy=save_strategy,
            save_steps=500,
            load_best_model_at_end=load_best_model_at_end,
            metric_for_best_model=metric_for_best_model,
            greater_is_better=greater_is_better,
            predict_with_generate=True
        )
        
        # Create data collator
        data_collator = DataCollatorForSeq2Seq(
            tokenizer=self.tokenizer,
            model=self.model
        )
        
        # Create trainer
        self.trainer = Seq2SeqTrainer(
            model=self.model,
            args=training_args,
            train_dataset=self.train_dataset,
            eval_dataset=self.eval_dataset,
            tokenizer=self.tokenizer,
            data_collator=data_collator
        )
        
        # Train the model
        logger.info("Starting training...")
        self.trainer.train()
        
        # Save the model
        self.trainer.save_model(output_dir)
        self.tokenizer.save_pretrained(output_dir)
        
        logger.info(f"Model saved to {output_dir}")
        
        return self.trainer
    
    def evaluate(self) -> Dict[str, float]:
        """
        Evaluate the trained model on the test dataset.
        
        Returns:
        --------
        dict : Evaluation metrics
        """
        if self.trainer is None:
            raise ValueError("Model not trained. Call train() first.")
        
        if self.test_dataset is None:
            raise ValueError("Test dataset not prepared. Call prepare_data_from_summaries() first.")
        
        logger.info("Evaluating model on test dataset...")
        metrics = self.trainer.evaluate(self.test_dataset)
        
        logger.info(f"Test metrics: {metrics}")
        return metrics
    
    def generate_summary(self, text: str) -> str:
        """
        Generate a summary for a given text.
        
        Parameters:
        -----------
        text : str
            Text to summarize
            
        Returns:
        --------
        str : Generated summary
        """
        # Tokenize input
        inputs = self.tokenizer(
            text, 
            max_length=self.max_input_length, 
            padding="max_length", 
            truncation=True, 
            return_tensors="pt"
        ).to(self.device)
        
        # Generate summary
        summary_ids = self.model.generate(
            inputs["input_ids"],
            max_length=self.max_output_length,
            num_beams=4,
            early_stopping=True
        )
        
        # Decode summary
        summary = self.tokenizer.decode(summary_ids[0], skip_special_tokens=True)
        
        return summary
    
    @classmethod
    def load_model(cls, model_path: str, device: Optional[str] = None) -> 'Seq2SeqSummarizerTrainer':
        """
        Load a trained model from disk.
        
        Parameters:
        -----------
        model_path : str
            Path to the saved model
        device : str, optional
            Device to load the model on
            
        Returns:
        --------
        Seq2SeqSummarizerTrainer : Loaded model
        """
        # Create a new instance
        trainer = cls(model_name=model_path, device=device)
        
        # Load tokenizer and model
        trainer.tokenizer = AutoTokenizer.from_pretrained(model_path)
        trainer.model = AutoModelForSeq2SeqLM.from_pretrained(model_path).to(trainer.device)
        
        logger.info(f"Model loaded from {model_path}")
        return trainer


# Example usage
if __name__ == "__main__":
    # Create a trainer instance
    trainer = Seq2SeqSummarizerTrainer(
        model_name="t5-small",
        max_input_length=512,
        max_output_length=128
    )
    
    # Load summaries (assuming they exist)
    try:
        summaries = trainer.load_summaries_from_file("generated_summaries/summaries_example.json")
        
        # Prepare data
        train_dataset, val_dataset, test_dataset = trainer.prepare_data_from_summaries(summaries)
        
        # Train the model
        trainer.train(
            output_dir="seq2seq_summarizer_model",
            num_train_epochs=3,
            per_device_train_batch_size=4,
            per_device_eval_batch_size=4
        )
        
        # Evaluate the model
        metrics = trainer.evaluate()
        
        # Generate a sample summary
        sample_text = "A computer hacker learns from mysterious rebels about the true nature of his reality and his role in the war against its controllers."
        summary = trainer.generate_summary(sample_text)
        print(f"Sample summary: {summary}")
        
    except FileNotFoundError:
        print("No summary files found. Please generate summaries using LLMSummarizer first.")
